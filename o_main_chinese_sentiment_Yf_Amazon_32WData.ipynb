{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ================\n",
    "### 在数据./git下载的数据集合/yf_amazon/ratings 抽取部分的数据来做情感分析\n",
    "### [5.0, 4.0, 1.0, 2.0, 3.0] - 打分列表\n",
    "### 小于3分的 - label =0\n",
    "### 大于3分的 - Label =1\n",
    "### 暂时先不考虑3分做为中性的问题\n",
    "### 2019-11-25\n",
    "### Liu,Yazhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import jieba\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential,model_from_yaml\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional,Dropout,Activation\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.models import save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.在原始数据中抽取样本和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './main _chinese_sentiment_datas/yf_amazon/ratings/'\n",
    "DataFrame = pd.read_csv(open(os.path.join(path, \"ratings.csv\"), 'r', \n",
    "                        encoding='utf-8'),\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 去掉任何行或者列为空的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame = DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 将评分+title+comments 存入文件32W数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIMITS    = 1e5*3.2  #32W 数据\n",
    "ratings  = DataFrame.rating.values.tolist()\n",
    "titles   = DataFrame.title.values.tolist()\n",
    "comments = DataFrame.comment.values.tolist() \n",
    "productIds = DataFrame.productId.values.tolist()\n",
    "writor = open('./main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titles_comments.txt','w',encoding='utf-8')\n",
    "writmerge = open('./main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments.txt','w',encoding='utf-8')\n",
    "for i in range(len(ratings)):\n",
    "    if i<=LIMITS:\n",
    "        writor.write(str(ratings[i])+'||'+titles[i]+'||'+comments[i]+'\\n')\n",
    "        writmerge.write(str(ratings[i])+'||'+titles[i]+','+comments[i]+'\\n')#将title+comments merge一起\n",
    "\n",
    "writor.close()\n",
    "writmerge.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.数据清洗后获得文本tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model\n",
    "\n",
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    texts  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",texts)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\;\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    texts = re.sub('\\·?\\－\\<\\·\\\\‘\\’\\?\\····.?\\···\\×\\=?','',texts)\n",
    "    texts = re.sub('\\－.?','',texts)\n",
    "    texts = re.sub('[^0-9A-Za-z\\u4e00-\\u9fa5]','',texts)# 留下中文，数字，大小写\n",
    "    return texts\n",
    "\n",
    "def get_train_data(path,texts_clean,texts_target,LENS = 1):\n",
    "    with open(path,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            items = line.split('||')\n",
    "      \n",
    "            text = items[1]\n",
    "            text = re_sub(text)\n",
    "            if len(text)>LENS and text!='\\n':\n",
    "                if eval(items[0])<3:\n",
    "                    texts_target.append(0)\n",
    "                    texts_clean.append(text)\n",
    "                elif eval(items[0])>3:\n",
    "                    texts_target.append(1)  ##### train_target 这里一定要存放数字，不能是字符串'1'/'0'\n",
    "                    texts_clean.append(text)\n",
    "                else:# items[0] == '3' # 中评\n",
    "                    pass\n",
    "                \n",
    "    return texts_target,texts_clean\n",
    "\n",
    "def get_train_tokens(train_tokens,texts_clean,cn_model,LENS =1):\n",
    "    for text in texts_clean:\n",
    "        cut = jieba.cut(text)\n",
    "        cut_list = [i for i in cut]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "            except KeyError:\n",
    "                cut_list[i] = 0\n",
    "        train_tokens.append(cut_list)\n",
    "    return train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cn_model = get_cn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments.txt'\n",
    "texts_clean = []\n",
    "texts_target= []\n",
    "train_tokens = []\n",
    "LENS = 2\n",
    "texts_target,texts_clean = get_train_data(path,texts_clean,texts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tokens = get_train_tokens(train_tokens,texts_clean,cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283935, 283935, 283935)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_clean),len(texts_target),len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我是这本书的亚马逊第个评论者的品质从内容设计到排版印刷均无懈可击我真喜欢我强烈推荐这本自然史'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_clean[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1将清洗后的文本及评分标签记录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments_clean.txt'\n",
    "writes = open(path,'w',encoding='utf-8')\n",
    "i= -1\n",
    "for line in texts_clean:\n",
    "    i+=1\n",
    "    writes.write(str(texts_target[i])+'||'+line+'\\n')\n",
    "writes.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.计算词汇表大小和词长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_token(train_tokens):\n",
    "    num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    return max_tokens\n",
    "\n",
    "def vocab_size(train_tokens):\n",
    "    vocab_tokens = []\n",
    "    i = 0\n",
    "    t1 = time.time()\n",
    "    for tokens in train_tokens:\n",
    "        for token in tokens:\n",
    "            i+=1\n",
    "            if token not in vocab_tokens:\n",
    "                vocab_tokens.append(token)\n",
    "            else:\n",
    "                pass\n",
    "        if i%(1e5) == 0:\n",
    "            print(\"处理前%d的词花费的时间是:%0.2f\"%(i,(time.time() - t1)/60),'mins')\n",
    "    print(\"词汇表大小是：\",len(vocab_tokens))\n",
    "    return len(vocab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = max_token(train_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前500000的词花费的时间是:0.34 mins\n",
      "处理前1200000的词花费的时间是:1.12 mins\n",
      "处理前4100000的词花费的时间是:5.13 mins\n",
      "处理前5800000的词花费的时间是:7.91 mins\n",
      "处理前6600000的词花费的时间是:9.34 mins\n",
      "词汇表大小是： 75249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75249"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = vocab_size(train_tokens)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 引入词嵌入矩阵获取特征并由样本Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_dim,num_words,cn_model):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for i in range(num_words):\n",
    "        embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "    embedding_matrix = embedding_matrix.astype('float32')\n",
    "    return embedding_matrix\n",
    "\n",
    "def add_padding(train_tokens,train_target,max_tokens,num_words):\n",
    "    train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    train_pad[train_pad>=num_words] = 0\n",
    "    train_targets = np.array(train_target)\n",
    "    return train_pad,train_targets\n",
    "def model(y_train,y_test,X_train,X_test,epochs,batch_size,num_classes):#mask_zero = True\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_tokens,\n",
    "                        trainable=False))##mask_zero= True,\n",
    "    model.add(Bidirectional(LSTM(units= 64, return_sequences=True)))\n",
    "    model.add(LSTM(units=16, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid')) # Dense全连接层，输出维度是3\n",
    "    #model.add(Activation('softmax'))\n",
    "    optimizer = Adam(lr=1*1e-2)\n",
    "    model.compile(loss='binary_crossentropy',#categorical_crossentropy\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    path_checkpoint = './main_model/sentiment_checkpoint_Amazon2cates.keras'\n",
    "    checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "    try:\n",
    "        model.load_weights(path_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    lr_reduction  = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "    callbacks = [earlystopping, checkpoint,lr_reduction]\n",
    "    #y_train_ = to_categorical(y_train,num_classes = num_classes)\n",
    "    #y_test_  = to_categorical(y_test,num_classes = num_classes)\n",
    "    \n",
    "    model.fit(X_train, y_train,validation_split=0.2, epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks)\n",
    "    from tensorflow.python.keras.models import save_model\n",
    "    save_model(model,'./main_model/sentiment_checkpoint_Amazon2cates.h5')\n",
    "    result = model.evaluate(X_test, y_test)\n",
    "    print('Accuracy is :{0:.2%}'.format(result[1]))\n",
    "    return result[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 训练前的数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_words     = 75249\n",
    "max_tokens    = 102\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(embedding_dim,num_words,cn_model)\n",
    "train_pad,texts_target_pad = add_padding(train_tokens,texts_target,max_tokens,num_words)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    texts_target,  # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283935, 283935, 283935)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens),len(texts_target),len(train_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.模型开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 300)          22574700  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         186880    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 22,770,877\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 22,574,700\n",
      "_________________________________________________________________\n",
      "Unable to open file (Unable to open file: name = './main_model/sentiment_checkpoint_amazon2cates.keras', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)\n",
      "Train on 181718 samples, validate on 45430 samples\n",
      "Epoch 1/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.1925 - acc: 0.9239Epoch 00000: val_loss improved from inf to 0.12140, saving model to ./main_model/sentiment_checkpoint_Amazon2cates.keras\n",
      "181718/181718 [==============================] - 915s - loss: 0.1925 - acc: 0.9239 - val_loss: 0.1214 - val_acc: 0.9555\n",
      "Epoch 2/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.1188 - acc: 0.9568Epoch 00001: val_loss improved from 0.12140 to 0.11437, saving model to ./main_model/sentiment_checkpoint_Amazon2cates.keras\n",
      "181718/181718 [==============================] - 935s - loss: 0.1187 - acc: 0.9568 - val_loss: 0.1144 - val_acc: 0.9585\n",
      "Epoch 3/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.1092 - acc: 0.9604Epoch 00002: val_loss improved from 0.11437 to 0.10713, saving model to ./main_model/sentiment_checkpoint_Amazon2cates.keras\n",
      "181718/181718 [==============================] - 1007s - loss: 0.1091 - acc: 0.9604 - val_loss: 0.1071 - val_acc: 0.9606\n",
      "Epoch 4/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.1008 - acc: 0.9640Epoch 00003: val_loss improved from 0.10713 to 0.10530, saving model to ./main_model/sentiment_checkpoint_Amazon2cates.keras\n",
      "181718/181718 [==============================] - 990s - loss: 0.1009 - acc: 0.9640 - val_loss: 0.1053 - val_acc: 0.9623\n",
      "Epoch 5/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.0956 - acc: 0.9663Epoch 00004: val_loss improved from 0.10530 to 0.10482, saving model to ./main_model/sentiment_checkpoint_Amazon2cates.keras\n",
      "181718/181718 [==============================] - 989s - loss: 0.0956 - acc: 0.9663 - val_loss: 0.1048 - val_acc: 0.9621\n",
      "Epoch 6/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.0885 - acc: 0.9695Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.0009999999776482583.\n",
      "181718/181718 [==============================] - 999s - loss: 0.0885 - acc: 0.9695 - val_loss: 0.1077 - val_acc: 0.9624\n",
      "Epoch 7/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.0759 - acc: 0.9750Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 9.999999310821295e-05.\n",
      "181718/181718 [==============================] - 1006s - loss: 0.0759 - acc: 0.9750 - val_loss: 0.1079 - val_acc: 0.9631\n",
      "Epoch 8/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.0704 - acc: 0.9775Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 9.999999019782991e-06.\n",
      "181718/181718 [==============================] - 998s - loss: 0.0704 - acc: 0.9775 - val_loss: 0.1085 - val_acc: 0.9631\n",
      "Epoch 9/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.0699 - acc: 0.9777Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 9.99999883788405e-07.\n",
      "181718/181718 [==============================] - 1009s - loss: 0.0699 - acc: 0.9777 - val_loss: 0.1086 - val_acc: 0.9632\n",
      "Epoch 10/10\n",
      "181248/181718 [============================>.] - ETA: 2s - loss: 0.0699 - acc: 0.9777Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 9.99999883788405e-08.\n",
      "181718/181718 [==============================] - 985s - loss: 0.0699 - acc: 0.9777 - val_loss: 0.1086 - val_acc: 0.9632\n",
      "56787/56787 [==============================] - 110s   \n",
      "Accuracy is :96.13%\n"
     ]
    }
   ],
   "source": [
    "epochs      = 10\n",
    "batch_size  = 512*2\n",
    "num_classes = 2\n",
    "max_tokens  = 102\n",
    "accuracy = model(y_train,y_test,X_train,X_test,epochs,batch_size,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 在个别新的样例上的测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model\n",
    "\n",
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    texts  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",texts)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\;\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    texts = re.sub('\\·?\\－\\<\\·\\\\‘\\’\\?\\····.?\\···\\×\\=?','',texts)\n",
    "    texts = re.sub('\\－.?','',texts)\n",
    "    texts = re.sub('[^0-9A-Za-z\\u4e00-\\u9fa5]','',texts)# 留下中文，数字，大小写\n",
    "    return texts\n",
    "\n",
    "def prediction(text,LENS =2):\n",
    "    cn_model             = get_cn_model()\n",
    "    text = re_sub(text)\n",
    "    if len(text)>LENS and text!='\\n' and text!='\\t':\n",
    "        cut_list = [i for i in jieba.cut(text)]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "                if cut_list[i]>=75249:  #nums word\n",
    "                    cut_list[i] = 0\n",
    "            except KeyError:\n",
    "                cut_list[i] = 0\n",
    "            \n",
    "    #padding\n",
    "    max_tokens = 102  #get from above model parameters\n",
    "    train_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    \n",
    "    # loading model\n",
    "    from tensorflow.python.keras.models import load_model\n",
    "    model_path = './main_model/sentiment_checkpoint_Amazon2cates.h5'\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    result = model.predict(x=train_pad)\n",
    "    \n",
    "    coefs = result[0][0]\n",
    "    return coefs*100  # 返回判断的阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.取一些样本测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_list= [\n",
    "    '垃圾完全一本垃圾也是垃圾',\n",
    "    '满意产品物流也快质量不错等着宝宝出了满月就可以用了',\n",
    "    '卓越快递服务太差卓越网的快递太差了服务太差了快递的宗旨不就是为了客户服务的吗',\n",
    "    '硬盘太小,好像不支持win7,8系统,盒上写得兼容性只有xp,2000,nt,是不是真的?',\n",
    "    '很实用,但要注意不是所有地漏都适用,家里地漏不适用,只能接别的地方了。产品设计不错,用料也可以,滤网也很有用。潜水艇的质量没的说。就是价格太贵了,所以只能三星推荐了'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for text in text_list:\n",
    "    res = prediction(text)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条评价是:垃圾完全一本垃圾也是垃圾\n",
      "模型判断这是一条: ##负面评价##,预测阈值是:3.94\n",
      "\n",
      "\n",
      "第2条评价是:满意产品物流也快质量不错等着宝宝出了满月就可以用了\n",
      "模型判断这是一条: ##正面评价##,预测阈值是:99.69\n",
      "\n",
      "\n",
      "第3条评价是:卓越快递服务太差卓越网的快递太差了服务太差了快递的宗旨不就是为了客户服务的吗\n",
      "模型判断这是一条: ##负面评价##,预测阈值是:32.22\n",
      "\n",
      "\n",
      "第4条评价是:硬盘太小,好像不支持win7,8系统,盒上写得兼容性只有xp,2000,nt,是不是真的?\n",
      "模型判断这是一条: ##负面评价##,预测阈值是:30.13\n",
      "\n",
      "\n",
      "第5条评价是:很实用,但要注意不是所有地漏都适用,家里地漏不适用,只能接别的地方了。产品设计不错,用料也可以,滤网也很有用。潜水艇的质量没的说。就是价格太贵了,所以只能三星推荐了\n",
      "模型判断这是一条: ##正面评价##,预测阈值是:99.72\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cte = ['正面评价','中性评价','负面评价']\n",
    "for i in range(len(text_list)):\n",
    "    if results[i]>50:\n",
    "        cte_j = 0\n",
    "    elif results[i]<50:\n",
    "        cte_j = 2\n",
    "    else:\n",
    "        cte_j = 1\n",
    "    print(\"第%d条评价是:%s\"%(i+1,text_list[i]) )\n",
    "    print(\"模型判断这是一条: ##%s##,预测阈值是:%0.2f\"%(cte[cte_j],results[i]) )\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 取一些额外的数据作为测试集合使用 =>效果较好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings  = DataFrame.rating.values.tolist()\n",
    "titles   = DataFrame.title.values.tolist()\n",
    "comments = DataFrame.comment.values.tolist() \n",
    "LIMITS    = 1e5*3.2\n",
    "writmerge = open('./main _chinese_sentiment_datas/yf_amazon/ratings/additional_Examples_2.txt','w',encoding='utf-8')\n",
    "for i in range(len(ratings)):\n",
    "    if i>LIMITS and i<LIMITS*1.5:\n",
    "        writmerge.write(str(ratings[i])+'||'+titles[i]+','+comments[i]+'\\n')\n",
    "        \n",
    "writmerge.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './main _chinese_sentiment_datas/yf_amazon/ratings/additional_Examples_2.txt'\n",
    "texts_clean = []\n",
    "texts_target= []\n",
    "train_tokens = []\n",
    "LENS = 2\n",
    "texts_target,texts_clean = get_train_data(path,texts_clean,texts_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新的测试样本个数: 140919\n"
     ]
    }
   ],
   "source": [
    "print(\"新的测试样本个数:\",len(texts_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tokens = get_train_tokens(train_tokens,texts_clean,cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_words     = 75249\n",
    "max_tokens    = 102\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(embedding_dim,num_words,cn_model)\n",
    "train_pad,texts_target_pad = add_padding(train_tokens,texts_target,max_tokens,num_words)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    texts_target,  # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './main_model/sentiment_checkpoint_Amazon2cates.h5'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28184/28184 [==============================] - 53s    \n",
      "Accuracy is :95.69%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy is :{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2加载模型后利用新的数据[11W]继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90188 samples, validate on 22547 samples\n",
      "Epoch 1/2\n",
      "90188/90188 [==============================] - 591s - loss: 0.1163 - acc: 0.9605 - val_loss: 0.1184 - val_acc: 0.9602\n",
      "Epoch 2/2\n",
      "90188/90188 [==============================] - 696s - loss: 0.1163 - acc: 0.9605 - val_loss: 0.1184 - val_acc: 0.9602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x28ce2fe2f98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,batch_size = 1024,validation_split = 0.2,epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28184/28184 [==============================] - 59s    \n",
      "\n",
      "New accuracy is :95.69%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('New accuracy is :{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import save_model\n",
    "save_model(model,'./main_model/sentiment_checkpoint_Amazon2cates.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
