{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 在10w中文微博数据上实现情感的二分类任务\n",
    "###  2019 - 11-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 用来解压\n",
    "import bz2\n",
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_token(train_tokens):\n",
    "    num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    return max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocab_size(train_tokens):\n",
    "    vocab_tokens = []\n",
    "    i = 0\n",
    "    for tokens in train_tokens:\n",
    "        for token in tokens:\n",
    "            i+=1\n",
    "            if token not in vocab_tokens:\n",
    "                vocab_tokens.append(token)\n",
    "            else:\n",
    "                pass\n",
    "    if i%1e6 == 0:\n",
    "        print(\"处理前%d的词花费的时间是:%0.2f\"%(i,(time.time() - t1)/60),'mins')\n",
    "    print(\"词汇表大小是：\",len(vocab_tokens))\n",
    "    return len(vocab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_dim,num_words,cn_model):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for i in range(num_words):\n",
    "        embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "    embedding_matrix = embedding_matrix.astype('float32')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_padding(train_tokens,train_target,max_tokens,num_words):\n",
    "    train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    train_pad[ train_pad>=num_words ] = 0\n",
    "    train_targets = np.array(train_target)\n",
    "    return train_pad,train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "    model.add(LSTM(units=16, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    path_checkpoint = './main_model/sentiment_checkpoint.keras'\n",
    "    checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "    try:\n",
    "        model.load_weights(path_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "    callbacks = [earlystopping, checkpoint,lr_reduction]\n",
    "    model.fit(X_train, y_train,validation_split=0.1, epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks)\n",
    "    result = model.evaluate(X_test, y_test)\n",
    "    print('Accuracy is :{0:.2%}'.format(result[1]))\n",
    "    return result[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 10W 数据开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    text  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",text)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    return texts\n",
    "\n",
    "def get_train_data(train_texts_orig,train_target):\n",
    "    path = './git下载的数据集合/weibo_senti_100k/weibo_senti_100k.txt'\n",
    "    with open(path,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            items = line.split(',')\n",
    "            train_texts_orig.append(items[1])\n",
    "\n",
    "            text = items[1]\n",
    "            text = re_sub(text)\n",
    "            if len(text)>LENS and text!='\\n':\n",
    "                if items[0] == '1':\n",
    "                    train_target.append(1)\n",
    "                else:\n",
    "                    train_target.append(0)  ##### train_target 这里一定要存放数字，不能是字符串'1'/'0'\n",
    "    return train_target,train_texts_orig\n",
    "\n",
    "def get_train_tokens(train_tokens,train_texts_orig,cn_model):\n",
    "    for text in train_texts_orig:\n",
    "        text = re_sub(text)\n",
    "        if len(text)>LENS and text!='\\n' and text!='\\t':\n",
    "            cut = jieba.cut(text)\n",
    "            cut_list = [i for i in cut]\n",
    "            for i, word in enumerate(cut_list):\n",
    "                try:\n",
    "                    cut_list[i] = cn_model.vocab[word].index\n",
    "                except KeyError:\n",
    "                    cut_list[i] = 0\n",
    "            train_tokens.append(cut_list)\n",
    "    return train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\H155809\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.720 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "LENS =1\n",
    "train_texts_orig = []\n",
    "train_tokens =[]\n",
    "cn_model = get_cn_model()\n",
    "train_target,train_texts_orig = get_train_data(train_texts_orig,train_target)\n",
    "train_tokens = get_train_tokens(train_tokens,train_texts_orig,cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('咱们结伴去布达佩斯当土豪吧！[嘻嘻]先备个攻略>>http://t.cn/zRoKlPj\\n',\n",
       " 1,\n",
       " [10682, 90, 1310, 37345, 1862, 3538, 610])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "nums  = random.randint(0,100)\n",
    "train_texts_orig[nums],train_target[nums],train_tokens[nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 56, 300)           22911000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         186880    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 23,107,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 22,911,000\n",
      "_________________________________________________________________\n",
      "Train on 96812 samples, validate on 10757 samples\n",
      "Epoch 1/6\n",
      "96256/96812 [============================>.] - ETA: 1s - loss: 0.1100 - acc: 0.9606Epoch 00000: val_loss improved from inf to 0.12512, saving model to ./main_model/sentiment_checkpoint.keras\n",
      "96812/96812 [==============================] - 219s - loss: 0.1099 - acc: 0.9607 - val_loss: 0.1251 - val_acc: 0.9574\n",
      "Epoch 2/6\n",
      "96256/96812 [============================>.] - ETA: 1s - loss: 0.1034 - acc: 0.9627Epoch 00001: val_loss did not improve\n",
      "\n",
      "Epoch 00001: reducing learning rate to 0.00010000000474974513.\n",
      "96812/96812 [==============================] - 218s - loss: 0.1035 - acc: 0.9626 - val_loss: 0.1259 - val_acc: 0.9569\n",
      "Epoch 3/6\n",
      "96256/96812 [============================>.] - ETA: 1s - loss: 0.0947 - acc: 0.9656Epoch 00002: val_loss did not improve\n",
      "\n",
      "Epoch 00002: reducing learning rate to 1.0000000474974514e-05.\n",
      "96812/96812 [==============================] - 216s - loss: 0.0948 - acc: 0.9656 - val_loss: 0.1263 - val_acc: 0.9569\n",
      "Epoch 4/6\n",
      "96256/96812 [============================>.] - ETA: 1s - loss: 0.0924 - acc: 0.9660Epoch 00003: val_loss did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 1.0000000656873453e-06.\n",
      "96812/96812 [==============================] - 217s - loss: 0.0924 - acc: 0.9661 - val_loss: 0.1266 - val_acc: 0.9570\n",
      "Epoch 5/6\n",
      "96256/96812 [============================>.] - ETA: 1s - loss: 0.0921 - acc: 0.9662Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 1.0000001111620805e-07.\n",
      "96812/96812 [==============================] - 217s - loss: 0.0921 - acc: 0.9663 - val_loss: 0.1267 - val_acc: 0.9570\n",
      "Epoch 6/6\n",
      "96256/96812 [============================>.] - ETA: 1s - loss: 0.0922 - acc: 0.9662Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 1.000000082740371e-08.\n",
      "96812/96812 [==============================] - 218s - loss: 0.0920 - acc: 0.9662 - val_loss: 0.1267 - val_acc: 0.9570\n",
      "11953/11953 [==============================] - 12s    \n",
      "Accuracy is :95.94%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "epochs     = 6      #迭代次数\n",
    "batch_size = 512*2  #每个batch大小\n",
    "LENS =1\n",
    "train_texts_orig = []\n",
    "train_target     = []\n",
    "train_tokens     = []\n",
    "if __name__ == '__main__':\n",
    "    cn_model                      = get_cn_model()\n",
    "    train_target,train_texts_orig = get_train_data(train_texts_orig,train_target)\n",
    "    train_tokens                  = get_train_tokens(train_tokens,train_texts_orig,cn_model)\n",
    "    #num_words                    = vocab_size(train_tokens) # 这个计算比较花费时间\n",
    "    num_words                     = 76370\n",
    "    max_tokens                    = max_token(train_tokens)\n",
    "    embedding_matrix              = get_embedding_matrix(embedding_dim,num_words,cn_model)\n",
    "    train_pad,train_targets       = add_padding(train_tokens,train_target,max_tokens,num_words)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    train_target, # array\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)\n",
    "    accuracy = model(epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上面模型预测准确率是:95.94 %\n"
     ]
    }
   ],
   "source": [
    "print(\"上面模型预测准确率是:%0.2f\"%accuracy,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119522, 56), (119522,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad.shape,train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
