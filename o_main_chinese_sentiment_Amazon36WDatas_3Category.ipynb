{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### ================\n",
    "### \n",
    "### [5.0, 4.0, 1.0, 2.0, 3.0] - 打分列表\n",
    "### 等于1分的 - label =0\n",
    "### 等于5分的 - Label =2\n",
    "### 等于3分的 - label =1\n",
    "### 2019-11-25\n",
    "### Liu,Yazhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import jieba\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential,model_from_yaml\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional,Dropout,Activation\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.models import save_model\n",
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.载入原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './main _chinese_sentiment_datas/yf_amazon/ratings/'\n",
    "DataFrame = pd.read_csv(open(os.path.join(path, \"ratings.csv\"), 'r', \n",
    "                        encoding='utf-8'),\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 去掉任何行或者列为空的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataFrame = DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15905.0</th>\n",
       "      <td>452609</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1380988800</td>\n",
       "      <td>很喜欢</td>\n",
       "      <td>很好很强大,纸张超赞不是一般画册所能比拟的,图片很好,特点基本都表现了出来。物种很全</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94522.0</th>\n",
       "      <td>452609</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1333123200</td>\n",
       "      <td>精品!</td>\n",
       "      <td>买过很多本DK的百科全书类图书,这本书是我买过的最经典的一本!很厚的一本书,装帧很漂亮!内容...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317087.0</th>\n",
       "      <td>452609</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1346688000</td>\n",
       "      <td>不错!!!!!!!!!!</td>\n",
       "      <td>没想到送货的时候正好是下雨天,而且书没有塑料包装。 但是送货人员还是很小心的,虽然外面的纸箱...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502593.0</th>\n",
       "      <td>452609</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1347638400</td>\n",
       "      <td>很值的一本书</td>\n",
       "      <td>是本很值得收藏的书。 本身DK的eyewitness系列就很值得收藏,这套书都秉承着科普、图...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098610.0</th>\n",
       "      <td>452609</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1349280000</td>\n",
       "      <td>本书书评</td>\n",
       "      <td>本书非常棒,经典就是不一样。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           productId  rating   timestamp         title  \\\n",
       "userId                                                   \n",
       "15905.0       452609     5.0  1380988800           很喜欢   \n",
       "94522.0       452609     5.0  1333123200           精品!   \n",
       "317087.0      452609     5.0  1346688000  不错!!!!!!!!!!   \n",
       "502593.0      452609     5.0  1347638400        很值的一本书   \n",
       "1098610.0     452609     5.0  1349280000          本书书评   \n",
       "\n",
       "                                                     comment  \n",
       "userId                                                        \n",
       "15905.0           很好很强大,纸张超赞不是一般画册所能比拟的,图片很好,特点基本都表现了出来。物种很全  \n",
       "94522.0    买过很多本DK的百科全书类图书,这本书是我买过的最经典的一本!很厚的一本书,装帧很漂亮!内容...  \n",
       "317087.0   没想到送货的时候正好是下雨天,而且书没有塑料包装。 但是送货人员还是很小心的,虽然外面的纸箱...  \n",
       "502593.0   是本很值得收藏的书。 本身DK的eyewitness系列就很值得收藏,这套书都秉承着科普、图...  \n",
       "1098610.0                                     本书非常棒,经典就是不一样。  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 将评分+title+comments 存入文件300W数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIMITS  = 1e4*300  #300W 数据 = 2966810\n",
    "ratings  = DataFrame.rating.values.tolist()\n",
    "titles   = DataFrame.title.values.tolist()\n",
    "comments = DataFrame.comment.values.tolist() \n",
    "productIds = DataFrame.productId.values.tolist()\n",
    "writor = open('./main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titles_comments_3.txt','w',encoding='utf-8')\n",
    "writmerge = open('./main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments_3.txt','w',encoding='utf-8')\n",
    "for i in range(len(ratings)):\n",
    "    if i<=LIMITS:\n",
    "        writor.write(str(ratings[i])+'||'+titles[i]+'||'+comments[i]+'\\n')\n",
    "        writmerge.write(str(ratings[i])+'||'+titles[i]+','+comments[i]+'\\n')#将title+comments merge一起\n",
    "\n",
    "writor.close()\n",
    "writmerge.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 在后100W数据中记录下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LIMITS  = 1e4*300  #300W 数据 = 2966810\n",
    "ratings  = DataFrame.rating.values.tolist()\n",
    "titles   = DataFrame.title.values.tolist()\n",
    "comments = DataFrame.comment.values.tolist() \n",
    "productIds = DataFrame.productId.values.tolist()\n",
    "\n",
    "writmerge = open('./main _chinese_sentiment_datas/yf_amazon/ratings/ratings_additional100W_3.txt','w',encoding='utf-8')\n",
    "for i in range(len(ratings)):\n",
    "    if i>LIMITS:\n",
    "        writmerge.write(str(ratings[i])+'||'+titles[i]+','+comments[i]+'\\n')#将title+comments merge一起\n",
    "\n",
    "writmerge.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43430, 123519, 607370)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m,j,k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(774319, 4081136, 43430)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "43430+123519+607370,len(ratings),j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.数据清洗后获得文本tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model\n",
    "\n",
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    texts  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",texts)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\;\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    texts = re.sub('\\·?\\－\\<\\·\\\\‘\\’\\?\\····.?\\···\\×\\=?','',texts)\n",
    "    texts = re.sub('\\－.?','',texts)\n",
    "    texts = re.sub('[^0-9A-Za-z\\u4e00-\\u9fa5]','',texts)# 留下中文，数字，大小写\n",
    "    return texts\n",
    "\n",
    "def get_train_data(path,texts_clean,texts_target,NUMS_FORONE_CATEGORY,LENS = 2):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    k = 0\n",
    "    with open(path,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            items = line.split('||')\n",
    "            text = items[1]\n",
    "            text = re_sub(text)\n",
    "            if len(text)>LENS and text!='\\n':\n",
    "                if eval(items[0])==1:\n",
    "                    i+=1\n",
    "                    if i<=NUMS_FORONE_CATEGORY:\n",
    "                        texts_target.append(0)\n",
    "                        texts_clean.append(text)\n",
    "                elif eval(items[0])==3:\n",
    "                    j+=1\n",
    "                    if j<=NUMS_FORONE_CATEGORY:\n",
    "                        texts_target.append(1)  #texts_target 出现数字\n",
    "                        texts_clean.append(text)\n",
    "                elif eval(items[0])==5:\n",
    "                    k+=1\n",
    "                    if k<=NUMS_FORONE_CATEGORY:\n",
    "                        texts_target.append(2)\n",
    "                        texts_clean.append(text)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    return texts_target,texts_clean\n",
    "\n",
    "def get_train_tokens(train_tokens,texts_clean,cn_model,LENS =1):\n",
    "    for text in texts_clean:\n",
    "        cut = jieba.cut(text)\n",
    "        cut_list = [i for i in cut]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "            except KeyError:\n",
    "                cut_list[i] = 0\n",
    "        train_tokens.append(cut_list)\n",
    "    return train_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cn_model = get_cn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1样本乱序处理 - 只运行一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 对原来的数据样本乱序 ### \n",
    "import random\n",
    "def shuffles(inputs,outputs):\n",
    "    contents = []\n",
    "    file1 = open(inputs, 'r', encoding='utf-8') \n",
    "    file2 = open(outputs, 'w', encoding='utf-8')\n",
    "    for line in file1.readlines():\n",
    "        contents.append(line)\n",
    "    \n",
    "    random.shuffle(contents)\n",
    "    for content in contents:\n",
    "        file2.write(content)\n",
    "    \n",
    "    file1.close()\n",
    "    file2.close()\n",
    "\n",
    "# 样本乱序后存放在\n",
    "raw_data = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments_3.txt'\n",
    "random_data = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments_3_random.txt'\n",
    "#shuffles(raw_data,random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 获取Tokens 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_data = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments_3.txt'\n",
    "texts_clean = []\n",
    "texts_target= []\n",
    "train_tokens = []\n",
    "NUMS_FORONE_CATEGORY = 1e4*12 #每个分类的数量控制住\n",
    "texts_target,texts_clean = get_train_data(random_data,texts_clean,texts_target,NUMS_FORONE_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\H155809\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.705 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "train_tokens = get_train_tokens(train_tokens,texts_clean,cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360000, 360000, 360000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_clean),len(texts_target),len(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 将清洗后的文本及评分标签记录下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_titlesmergecomments_3_clean.txt'\n",
    "writes = open(path,'w',encoding='utf-8')\n",
    "i= -1\n",
    "for line in texts_clean:\n",
    "    i+=1\n",
    "    writes.write(str(texts_target[i])+'||'+line+'\\n')\n",
    "writes.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.计算词汇表大小和词长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_token(train_tokens):\n",
    "    num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    return max_tokens\n",
    "\n",
    "def vocab_size(train_tokens):\n",
    "    vocab_tokens = []\n",
    "    i = 0\n",
    "    t1 = time.time()\n",
    "    for tokens in train_tokens:\n",
    "        for token in tokens:\n",
    "            i+=1\n",
    "            if token not in vocab_tokens:\n",
    "                vocab_tokens.append(token)\n",
    "            else:\n",
    "                pass\n",
    "        if i%(1e5) == 0:\n",
    "            print(\"处理前%d的词花费的时间是:%0.2f\"%(i,(time.time() - t1)/60),'mins')\n",
    "    print(\"词汇表大小是：\",len(vocab_tokens))\n",
    "    return len(vocab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = max_token(train_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前400000的词花费的时间是:0.24 mins\n",
      "处理前3900000的词花费的时间是:4.65 mins\n",
      "处理前6200000的词花费的时间是:7.96 mins\n",
      "处理前6900000的词花费的时间是:9.03 mins\n",
      "处理前7600000的词花费的时间是:10.08 mins\n",
      "处理前7800000的词花费的时间是:10.38 mins\n",
      "处理前9000000的词花费的时间是:12.42 mins\n",
      "词汇表大小是： 85427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85427"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = vocab_size(train_tokens)\n",
    "num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 引入词嵌入矩阵获取特征并由样本Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_dim,num_words,cn_model):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for i in range(num_words):\n",
    "        embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "    embedding_matrix = embedding_matrix.astype('float32')\n",
    "    return embedding_matrix\n",
    "\n",
    "def add_padding(train_tokens,train_target,max_tokens,num_words):\n",
    "    train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    train_pad[train_pad>=num_words] = 0\n",
    "    train_targets = np.array(train_target)\n",
    "    return train_pad,train_targets\n",
    "def model(y_train,y_test,X_train,X_test,epochs,batch_size,num_classes):#mask_zero = True\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                        embedding_dim,\n",
    "                        mask_zero= True,\n",
    "                        weights=[embedding_matrix],\n",
    "                        input_length=max_tokens,\n",
    "                        trainable=False))##,\n",
    "    model.add(Bidirectional(LSTM(units= 64, return_sequences=True)))\n",
    "    model.add(LSTM(units=16, return_sequences=False))\n",
    "    model.add(Dense(3, activation='softmax')) # Dense全连接层，输出维度是3\n",
    "    #model.add(Activation('softmax'))\n",
    "    optimizer = Adam(lr=1*1e-2)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    path_checkpoint = './main_model/sentiment_checkpoint_Amazon3cates.keras'\n",
    "    checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "    try:\n",
    "        model.load_weights(path_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    lr_reduction  = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "    callbacks = [earlystopping, checkpoint,lr_reduction]\n",
    "    y_train_ = to_categorical(y_train,num_classes = num_classes)\n",
    "    y_test_  = to_categorical(y_test,num_classes = num_classes)\n",
    "    \n",
    "    model.fit(X_train, y_train_,validation_split=0.2, epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks)\n",
    "    \n",
    "    save_model(model,'./main_model/sentiment_checkpoint_Amazon3cates.h5')\n",
    "    result = model.evaluate(X_test, y_test_)\n",
    "    print('\\nAccuracy is :{0:.2%}'.format(result[1]))\n",
    "    return result[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 训练前的数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_words     = 85427\n",
    "max_tokens    = 111\n",
    "\n",
    "embedding_matrix = get_embedding_matrix(embedding_dim,num_words,cn_model)\n",
    "train_pad,texts_target_pad = add_padding(train_tokens,texts_target,max_tokens,num_words)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    texts_target,  # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360000, 360000, 360000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens),len(texts_target),len(train_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.模型开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs      = 2\n",
    "batch_size  = 1024\n",
    "num_classes = 3\n",
    "#accuracy = model(y_train,y_test,X_train,X_test,epochs,batch_size,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.重新加载模型 - 继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = './main_model/sentiment_checkpoint_Amazon3cates.h5'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========epochs========== ===> 1\n",
      "Train on 230400 samples, validate on 57600 samples\n",
      "Epoch 1/1\n",
      "230400/230400 [==============================] - 1652s - loss: 0.3748 - acc: 0.8616 - val_loss: 0.3733 - val_acc: 0.8622\n",
      "72000/72000 [==============================] - 183s   \n",
      "\n",
      "Accuracy is :86.08%\n",
      "\n",
      "=========epochs========== ===> 3\n",
      "Train on 230400 samples, validate on 57600 samples\n",
      "Epoch 1/3\n",
      "230400/230400 [==============================] - 1698s - loss: 0.3739 - acc: 0.8621 - val_loss: 0.3698 - val_acc: 0.8628\n",
      "Epoch 2/3\n",
      "230400/230400 [==============================] - 1728s - loss: 0.3737 - acc: 0.8623 - val_loss: 0.3698 - val_acc: 0.8628\n",
      "Epoch 3/3\n",
      "230400/230400 [==============================] - 1801s - loss: 0.3735 - acc: 0.8623 - val_loss: 0.3698 - val_acc: 0.8629\n",
      "72000/72000 [==============================] - 185s   \n",
      "\n",
      "Accuracy is :85.90%\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 1024\n",
    "num_classes = 3 \n",
    "model_path = './main_model/sentiment_checkpoint_Amazon3cates.h5'\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    if i%2 ==0:\n",
    "        time.sleep(60*2)\n",
    "    else:\n",
    "        print('\\n=========epochs========== ===>',i)\n",
    "        model = load_model(model_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    texts_target,  # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=50*i + i)\n",
    "        y_train_ = to_categorical(y_train,num_classes = num_classes)\n",
    "        y_test_  = to_categorical(y_test,num_classes = num_classes)\n",
    "        model.fit(X_train, y_train_,validation_split=0.2, epochs=i,\n",
    "          batch_size=batch_size)\n",
    "        save_model(model,'./main_model/sentiment_checkpoint_Amazon3cates.h5')\n",
    "        result = model.evaluate(X_test, y_test_)\n",
    "        print('\\nAccuracy is :{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(model,'./main_model/sentiment_checkpoint_Amazon3cates.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000/72000 [==============================] - 199s   \n",
      "\n",
      "Accuracy is :85.90%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test_)\n",
    "print('Accuracy is :{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 在个别新的样例上测试结果 =>效果不错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model\n",
    "\n",
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    texts  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",texts)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\;\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    texts = re.sub('\\·?\\－\\<\\·\\\\‘\\’\\?\\····.?\\···\\×\\=?','',texts)\n",
    "    texts = re.sub('\\－.?','',texts)\n",
    "    texts = re.sub('[^0-9A-Za-z\\u4e00-\\u9fa5]','',texts)# 留下中文，数字，大小写\n",
    "    return texts\n",
    "\n",
    "def prediction(text,LENS =2):\n",
    "    cn_model             = get_cn_model()\n",
    "    text = re_sub(text)\n",
    "    if len(text)>LENS and text!='\\n' and text!='\\t':\n",
    "        cut_list = [i for i in jieba.cut(text)]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "                if cut_list[i]>=85427:  #nums word\n",
    "                    cut_list[i] = 0\n",
    "            except KeyError:\n",
    "                cut_list[i] = 0\n",
    "            \n",
    "    #padding\n",
    "    max_tokens = 111  #get from above model parameters\n",
    "    train_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    \n",
    "    # loading model\n",
    "    from tensorflow.python.keras.models import load_model\n",
    "    model_path = './main_model/sentiment_checkpoint_Amazon3cates.h5'\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    #result = model.predict(x=train_pad)\n",
    "    label = model.predict_classes(x=train_pad)\n",
    "    \n",
    "    #coefs = result[0][0]\n",
    "    return label[0]  # 返回判断的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 1s\n",
      "1/1 [==============================] - 1s\n",
      "1/1 [==============================] - 1s\n",
      "1/1 [==============================] - 1s\n",
      "1/1 [==============================] - 1s\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "    # 1\n",
    "    '颜色不好看,送给我的婆婆和妈妈了。不适合我',\n",
    "    '退都不能退!!!,根本不是所谓的淡紫色,明明就是咖色',\n",
    "    '很垃圾的产品,根本就没有大家评论的那么好 怀疑她们都是托 图到嘴唇上很粘稠 根本就涂不开 什么光泽度也没有 大家不要听那些好评的 我花了44买的真亏 打水漂了',\n",
    "    '颜色不好,不好用,颜色不好看,直接丢了',\n",
    "    # 3\n",
    "    '差强人意,颜色不好看,开始我还以为发错颜色了,结果一对比膏体上的货号名称,确认就是樱桃红,一点都不像樱桃那种红色,倒有点像玫瑰红,暗沉,不像樱桃那么娇艳,不喜欢,但是包装和送货还是不错的,总之差强人意吧!',\n",
    "    '颜色不是特别喜欢,买回来后,觉得跟图示的颜色有出入,颜色不是特别喜欢,一直没用',\n",
    "    '不是很好,不是特别喜欢这个色,选 错了号了',\n",
    "    '露华浓,不是唇蜜。。不是我想象中的效果。。。不是很好。',\n",
    "    # 5\n",
    "    '颜色还可以,颜色还可以就是有点深,质感挺好的。',\n",
    "    '喜欢,女儿说挺好用的,有点闪亮闪亮的,颜色也挺适合!',\n",
    "    '颜色很正,感觉比较好用,很方便',\n",
    "    '非常喜欢啊!很美呢?,和板友去玩的时候涂的这个,都说好看,我个人是非常喜欢的,玩滑板都特别起劲。哈哈!颜色超级无敌美。'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for text in text_list:\n",
    "    label = prediction(text)\n",
    "    results.append(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条评价是:颜色不好看,送给我的婆婆和妈妈了。不适合我\n",
      "模型判断这是一条: ##负面评价##\n",
      "\n",
      "\n",
      "第2条评价是:退都不能退!!!,根本不是所谓的淡紫色,明明就是咖色\n",
      "模型判断这是一条: ##负面评价##\n",
      "\n",
      "\n",
      "第3条评价是:很垃圾的产品,根本就没有大家评论的那么好 怀疑她们都是托 图到嘴唇上很粘稠 根本就涂不开 什么光泽度也没有 大家不要听那些好评的 我花了44买的真亏 打水漂了\n",
      "模型判断这是一条: ##负面评价##\n",
      "\n",
      "\n",
      "第4条评价是:颜色不好,不好用,颜色不好看,直接丢了\n",
      "模型判断这是一条: ##负面评价##\n",
      "\n",
      "\n",
      "第5条评价是:差强人意,颜色不好看,开始我还以为发错颜色了,结果一对比膏体上的货号名称,确认就是樱桃红,一点都不像樱桃那种红色,倒有点像玫瑰红,暗沉,不像樱桃那么娇艳,不喜欢,但是包装和送货还是不错的,总之差强人意吧!\n",
      "模型判断这是一条: ##中性评价##\n",
      "\n",
      "\n",
      "第6条评价是:颜色不是特别喜欢,买回来后,觉得跟图示的颜色有出入,颜色不是特别喜欢,一直没用\n",
      "模型判断这是一条: ##中性评价##\n",
      "\n",
      "\n",
      "第7条评价是:不是很好,不是特别喜欢这个色,选 错了号了\n",
      "模型判断这是一条: ##中性评价##\n",
      "\n",
      "\n",
      "第8条评价是:露华浓,不是唇蜜。。不是我想象中的效果。。。不是很好。\n",
      "模型判断这是一条: ##中性评价##\n",
      "\n",
      "\n",
      "第9条评价是:颜色还可以,颜色还可以就是有点深,质感挺好的。\n",
      "模型判断这是一条: ##中性评价##\n",
      "\n",
      "\n",
      "第10条评价是:喜欢,女儿说挺好用的,有点闪亮闪亮的,颜色也挺适合!\n",
      "模型判断这是一条: ##正面评价##\n",
      "\n",
      "\n",
      "第11条评价是:颜色很正,感觉比较好用,很方便\n",
      "模型判断这是一条: ##正面评价##\n",
      "\n",
      "\n",
      "第12条评价是:非常喜欢啊!很美呢?,和板友去玩的时候涂的这个,都说好看,我个人是非常喜欢的,玩滑板都特别起劲。哈哈!颜色超级无敌美。\n",
      "模型判断这是一条: ##正面评价##\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cte = {2:'正面评价',1:'中性评价',0:'负面评价'}\n",
    "i = 0\n",
    "for label in results:\n",
    "    i+=1\n",
    "    print(\"第%d条评价是:%s\"%(i,text_list[i-1]) )\n",
    "    print(\"模型判断这是一条: ##%s##\"%(cte[label]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 取一些额外的数据[13W+]继续训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_new = './main _chinese_sentiment_datas/yf_amazon/ratings/ratings_additional12W_3.txt'\n",
    "\n",
    "texts_clean = []\n",
    "texts_target= []\n",
    "train_tokens = []\n",
    "texts_target,texts_clean = get_train_data(path_new,texts_clean,texts_target,43148,LENS = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_tokens = get_train_tokens(train_tokens,texts_clean,cn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129444, 129444, 129444)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_clean),len(texts_target),len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "num_words     = 85427\n",
    "max_tokens    = 111\n",
    "embedding_matrix = get_embedding_matrix(embedding_dim,num_words,cn_model)\n",
    "train_pad,texts_target_pad = add_padding(train_tokens,texts_target,max_tokens,num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= epochs =============== 1\n",
      "Train on 93751 samples, validate on 10417 samples\n",
      "Epoch 1/1\n",
      "93751/93751 [==============================] - 769s - loss: 0.4760 - acc: 0.8161 - val_loss: 0.4603 - val_acc: 0.8218\n",
      "26043/26043 [==============================] - 69s    \n",
      "Accuracy is :81.91%\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 1024\n",
    "num_classes = 3 \n",
    "model_path = './main_model/sentiment_checkpoint_Amazon3cates.h5'\n",
    "\n",
    "\n",
    "for i in range(1,3):\n",
    "    if i%2 ==0:\n",
    "        time.sleep(60*2)\n",
    "    else:\n",
    "        print('\\n========= epochs ===============',i)\n",
    "        model = load_model(model_path)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    texts_target,  # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=560*i + i)\n",
    "        y_train_ = to_categorical(y_train,num_classes = num_classes)\n",
    "        y_test_  = to_categorical(y_test,num_classes = num_classes)\n",
    "        model.fit(X_train, y_train_,validation_split=0.1, epochs=i,\n",
    "          batch_size=batch_size)\n",
    "        save_model(model,'./main_model/sentiment_checkpoint_Amazon3cates.h5')\n",
    "        result = model.evaluate(X_test, y_test_)\n",
    "        print('Accuracy is :{0:.2%}'.format(result[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
