{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 在10w中文微博数据上实现情感的二分类任务\n",
    "###  2019 - 11-19\n",
    "## LIu Yazhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 用来解压\n",
    "import bz2\n",
    "# 我们使用tensorflow的keras接口来建模\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_token(train_tokens):\n",
    "    num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    return max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocab_size(train_tokens):\n",
    "    vocab_tokens = []\n",
    "    i = 0\n",
    "    for tokens in train_tokens:\n",
    "        for token in tokens:\n",
    "            i+=1\n",
    "            if token not in vocab_tokens:\n",
    "                vocab_tokens.append(token)\n",
    "            else:\n",
    "                pass\n",
    "    if i%1e6 == 0:\n",
    "        print(\"处理前%d的词花费的时间是:%0.2f\"%(i,(time.time() - t1)/60),'mins')\n",
    "    print(\"词汇表大小是：\",len(vocab_tokens))\n",
    "    return len(vocab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(embedding_dim,num_words,cn_model):\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for i in range(num_words):\n",
    "        embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "    embedding_matrix = embedding_matrix.astype('float32')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_padding(train_tokens,train_target,max_tokens,num_words):\n",
    "    train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    train_pad[ train_pad>=num_words ] = 0\n",
    "    train_targets = np.array(train_target)\n",
    "    return train_pad,train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(epochs,batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "    model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "    model.add(LSTM(units=16, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    path_checkpoint = './main_model/sentiment_checkpoint10.keras'\n",
    "    checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "    try:\n",
    "        model.load_weights(path_checkpoint)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "    callbacks = [earlystopping, checkpoint,lr_reduction]\n",
    "    model.fit(X_train, y_train,validation_split=0.2, epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks)\n",
    "    \n",
    "    from tensorflow.python.keras.models import save_model\n",
    "    save_model(model,'./main_model/sentiment_checkpoint10.h5')\n",
    "    result = model.evaluate(X_test, y_test)\n",
    "    print(' Accuracy is :{0:.2%}'.format(result[1]))\n",
    "    return result[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1. 10W 数据的预处理从这里开始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1.1 样本乱序\n",
    "### 对原来的数据样本乱序 ### \n",
    "import random\n",
    "def shuffles(inputs,outputs):\n",
    "    contents = []\n",
    "    file1 = open(inputs, 'r', encoding='utf-8') \n",
    "    file2 = open(outputs, 'w', encoding='utf-8')\n",
    "    for line in file1.readlines():\n",
    "        contents.append(line)\n",
    "    \n",
    "    random.shuffle(contents)\n",
    "    for content in contents:\n",
    "        file2.write(content)\n",
    "    \n",
    "    file1.close()\n",
    "    file2.close()\n",
    "\n",
    "path = './data/weibo_senti_100k/weibo_senti_100k.txt'\n",
    "output = './data/weibo_senti_100k/weibo_senti_100k_random.txt'\n",
    "#shuffles(path,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    text  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",text)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    return texts\n",
    "\n",
    "def get_train_data(train_texts_orig,train_target):\n",
    "    path = './data/weibo_senti_100k/weibo_senti_100k_random.txt'\n",
    "    with open(path,'r',encoding = 'utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            items = line.split(',')\n",
    "            train_texts_orig.append(items[1])\n",
    "\n",
    "            text = items[1]\n",
    "            text = re_sub(text)\n",
    "            if len(text)>LENS and text!='\\n':\n",
    "                if items[0] == '1':\n",
    "                    train_target.append(1)\n",
    "                else:\n",
    "                    train_target.append(0)  ##### train_target 这里一定要存放数字，不能是字符串'1'/'0'\n",
    "    return train_target,train_texts_orig\n",
    "\n",
    "def get_train_tokens(train_tokens,train_texts_orig,cn_model):\n",
    "    for text in train_texts_orig:\n",
    "        text = re_sub(text)\n",
    "        if len(text)>LENS and text!='\\n' and text!='\\t':\n",
    "            cut = jieba.cut(text)\n",
    "            cut_list = [i for i in cut]\n",
    "            for i, word in enumerate(cut_list):\n",
    "                try:\n",
    "                    cut_list[i] = cn_model.vocab[word].index\n",
    "                except KeyError:\n",
    "                    cut_list[i] = 0\n",
    "            train_tokens.append(cut_list)\n",
    "    return train_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.运行主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\H155809\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.713 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 56, 300)           22911000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         186880    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 23,107,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 22,911,000\n",
      "_________________________________________________________________\n",
      "Unable to open file (Unable to open file: name = './main_model/sentiment_checkpoint10.keras', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)\n",
      "Train on 76493 samples, validate on 19124 samples\n",
      "Epoch 1/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.3386 - acc: 0.8603Epoch 00000: val_loss improved from inf to 0.23349, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 185s - loss: 0.3375 - acc: 0.8608 - val_loss: 0.2335 - val_acc: 0.9097\n",
      "Epoch 2/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.2139 - acc: 0.9190Epoch 00001: val_loss improved from 0.23349 to 0.19565, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 194s - loss: 0.2140 - acc: 0.9190 - val_loss: 0.1957 - val_acc: 0.9266\n",
      "Epoch 3/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1829 - acc: 0.9318Epoch 00002: val_loss improved from 0.19565 to 0.17144, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 200s - loss: 0.1828 - acc: 0.9318 - val_loss: 0.1714 - val_acc: 0.9352\n",
      "Epoch 4/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1581 - acc: 0.9424Epoch 00003: val_loss improved from 0.17144 to 0.15620, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 196s - loss: 0.1580 - acc: 0.9424 - val_loss: 0.1562 - val_acc: 0.9423\n",
      "Epoch 5/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1416 - acc: 0.9498Epoch 00004: val_loss improved from 0.15620 to 0.14762, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 191s - loss: 0.1415 - acc: 0.9498 - val_loss: 0.1476 - val_acc: 0.9478\n",
      "Epoch 6/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1315 - acc: 0.9539Epoch 00005: val_loss improved from 0.14762 to 0.13895, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 195s - loss: 0.1316 - acc: 0.9539 - val_loss: 0.1389 - val_acc: 0.9508\n",
      "Epoch 7/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1213 - acc: 0.9579Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 0.00010000000474974513.\n",
      "76493/76493 [==============================] - 192s - loss: 0.1210 - acc: 0.9581 - val_loss: 0.1391 - val_acc: 0.9497\n",
      "Epoch 8/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1124 - acc: 0.9604Epoch 00007: val_loss improved from 0.13895 to 0.13280, saving model to ./main_model/sentiment_checkpoint10.keras\n",
      "76493/76493 [==============================] - 193s - loss: 0.1124 - acc: 0.9604 - val_loss: 0.1328 - val_acc: 0.9542\n",
      "Epoch 9/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1100 - acc: 0.9618Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 1.0000000474974514e-05.\n",
      "76493/76493 [==============================] - 196s - loss: 0.1101 - acc: 0.9617 - val_loss: 0.1329 - val_acc: 0.9541\n",
      "Epoch 10/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1088 - acc: 0.9621Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 1.0000000656873453e-06.\n",
      "76493/76493 [==============================] - 195s - loss: 0.1088 - acc: 0.9622 - val_loss: 0.1328 - val_acc: 0.9542\n",
      "Epoch 11/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1087 - acc: 0.9622Epoch 00010: val_loss did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 1.0000001111620805e-07.\n",
      "76493/76493 [==============================] - 191s - loss: 0.1086 - acc: 0.9622 - val_loss: 0.1328 - val_acc: 0.9542\n",
      "Epoch 12/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1084 - acc: 0.9623Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 1.000000082740371e-08.\n",
      "76493/76493 [==============================] - 189s - loss: 0.1086 - acc: 0.9622 - val_loss: 0.1328 - val_acc: 0.9542\n",
      "Epoch 13/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1088 - acc: 0.9622Epoch 00012: val_loss did not improve\n",
      "76493/76493 [==============================] - 189s - loss: 0.1086 - acc: 0.9622 - val_loss: 0.1328 - val_acc: 0.9542\n",
      "Epoch 14/20\n",
      "75776/76493 [============================>.] - ETA: 1s - loss: 0.1087 - acc: 0.9622Epoch 00013: val_loss did not improve\n",
      "76493/76493 [==============================] - 193s - loss: 0.1086 - acc: 0.9622 - val_loss: 0.1328 - val_acc: 0.9542\n",
      "Epoch 00013: early stopping\n",
      "23872/23905 [============================>.] - ETA: 0sAccuracy is :95.60%\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 300\n",
    "epochs     = 20      #迭代次数\n",
    "batch_size = 512*2  #每个batch大小\n",
    "LENS =1\n",
    "train_texts_orig = []\n",
    "train_target     = []\n",
    "train_tokens     = []\n",
    "if __name__ == '__main__':\n",
    "    cn_model                      = get_cn_model()\n",
    "    train_target,train_texts_orig = get_train_data(train_texts_orig,train_target)\n",
    "    train_tokens                  = get_train_tokens(train_tokens,train_texts_orig,cn_model)\n",
    "    #num_words                    = vocab_size(train_tokens) # 这个计算比较花费时间\n",
    "    num_words                     = 76370\n",
    "    max_tokens                    = max_token(train_tokens)\n",
    "    embedding_matrix              = get_embedding_matrix(embedding_dim,num_words,cn_model)\n",
    "    train_pad,train_targets       = add_padding(train_tokens,train_target,max_tokens,num_words)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_pad, # array\n",
    "                                                    train_target, # array\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=660)\n",
    "    accuracy = model(epochs,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上面模型预测准确率是:95.60 %\n"
     ]
    }
   ],
   "source": [
    "print(\"上面模型预测准确率是:%0.2f\"%accuracy,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119522, 56), (119522,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pad.shape,train_targets.shape  #样本大小 - 大约12w的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. 模型预测新的样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cn_model(): \n",
    "    cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "    \n",
    "    return cn_model\n",
    "def re_sub(inputs):\n",
    "    ## inputs is one list and outputs is also\n",
    "    text  = inputs\n",
    "    texts = re.sub(\"\\//@[a-zA-Z\\W+]+\", \"\",text)\n",
    "    texts = re.sub(\"\\@[a-zA-Z\\w+]+\", \"\",texts)\n",
    "    texts = re.sub(\"[\\-\\#+\\//@.\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\",\"\",texts)\n",
    "    texts = re.sub('[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\:\\::\\?\\“\\”\\”“\\～+\\:?\\;;\\>>]','',texts)\n",
    "    texts = re.sub('\\：：?','',texts)\n",
    "    return texts\n",
    "\n",
    "def prediction(text,LENS =1):\n",
    "    cn_model             = get_cn_model()\n",
    "    text = re_sub(text)\n",
    "    if len(text)>LENS and text!='\\n' and text!='\\t':\n",
    "        cut_list = [i for i in jieba.cut(text)]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "                if cut_list[i]>=76370:  #nums word\n",
    "                    cut_list[i] = 0\n",
    "            except KeyError:\n",
    "                cut_list[i] = 0\n",
    "            \n",
    "    #padding\n",
    "    max_tokens = 56  #get from above model parameters\n",
    "    train_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')\n",
    "    \n",
    "    # loading model\n",
    "    from tensorflow.python.keras.models import load_model\n",
    "    model_path = './main_model/sentiment_checkpoint10.h5'\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    result = model.predict(x=train_pad)\n",
    "    \n",
    "    coefs = result[0][0]\n",
    "    return coefs*100  # 返回判断的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_list= [\n",
    "    '我今天为什么要穿白长裙？[泪]还有几站该下车了！雨依然哗哗的！[抓狂] //@败家de小妞子:裙子已经湿了[泪]',\n",
    "    '糟糕透顶了，刚买的新手机就丢厕所里面了,想骂人呀，谁也别招惹我。',\n",
    "    '今天是个好日子，天气特别美好，我的心情也很好',\n",
    "    '新发的工资,还意外的领到了红包',\n",
    "    '朋友出车祸了',\n",
    "    '今天去出差，住豪华酒店',\n",
    "    '今天第一次陪朋友去逛街',\n",
    "    '这不科学……应该是一堆狗~//@蜘蛛3号: //@我的朋友是个呆B:QAQ//@进击的巨人官网:QAQ//@我的同事是个婊子: QAQ//@',\n",
    "    '我刚问了老公这个问题，宝宝出生后他会说什么，他不加思索地来了一句：“八喜，欢迎来到地球！”[汗][晕]'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for text in text_list:\n",
    "    res = prediction(text)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1条微博是:我今天为什么要穿白长裙？[泪]还有几站该下车了！雨依然哗哗的！[抓狂] //@败家de小妞子:裙子已经湿了[泪]\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:0.14\n",
      "\n",
      "\n",
      "第2条微博是:糟糕透顶了，刚买的新手机就丢厕所里面了,想骂人呀，谁也别招惹我。\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:1.03\n",
      "\n",
      "\n",
      "第3条微博是:今天是个好日子，天气特别美好，我的心情也很好\n",
      "模型判断这是一条: ##正面情绪##,预测阈值是:59.76\n",
      "\n",
      "\n",
      "第4条微博是:新发的工资,还意外的领到了红包\n",
      "模型判断这是一条: ##正面情绪##,预测阈值是:50.34\n",
      "\n",
      "\n",
      "第5条微博是:朋友出车祸了\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:8.78\n",
      "\n",
      "\n",
      "第6条微博是:今天去出差，住豪华酒店\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:3.31\n",
      "\n",
      "\n",
      "第7条微博是:今天第一次陪朋友去逛街\n",
      "模型判断这是一条: ##正面情绪##,预测阈值是:55.14\n",
      "\n",
      "\n",
      "第8条微博是:这不科学……应该是一堆狗~//@蜘蛛3号: //@我的朋友是个呆B:QAQ//@进击的巨人官网:QAQ//@我的同事是个婊子: QAQ//@\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:0.84\n",
      "\n",
      "\n",
      "第9条微博是:我刚问了老公这个问题，宝宝出生后他会说什么，他不加思索地来了一句：“八喜，欢迎来到地球！”[汗][晕]\n",
      "模型判断这是一条: ##负面情绪##,预测阈值是:0.14\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cte = ['正面情绪','负面情绪']\n",
    "for i in range(len(text_list)):\n",
    "    \n",
    "    if results[i]>50:\n",
    "        cte_j = 0\n",
    "    else:\n",
    "        cte_j = 1\n",
    "    print(\"第%d条微博是:%s\"%(i+1,text_list[i]) )\n",
    "    print(\"模型判断这是一条: ##%s##,预测阈值是:%0.2f\"%(cte[cte_j],results[i]) )\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
