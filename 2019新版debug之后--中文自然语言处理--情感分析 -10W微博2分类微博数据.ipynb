{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 首先加载必用的库\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import jieba # 结巴分词\n",
    "# gensim用来加载预训练word vector\n",
    "from gensim.models import KeyedVectors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 用来解压\n",
    "import bz2\n",
    "import tensorflow\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import fasttext\n",
    "import sklearn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预训练词向量**  \n",
    "本教程使用了北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\" github链接为：  \n",
    "https://github.com/Embedding/Chinese-Word-Vectors  \n",
    "如果你不知道word2vec是什么，我推荐以下一篇文章：  \n",
    "https://zhuanlan.zhihu.com/p/26306795  \n",
    "这里我们使用了\"chinese-word-vectors\"知乎Word + Ngram的词向量，可以从上面github链接下载，我们先加载预训练模型并进行一些简单测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花费时间： 0.6230101784070333 mins\n"
     ]
    }
   ],
   "source": [
    "# 请将下载的词向量压缩包放置在根目录 embeddings 文件夹里\n",
    "# 解压词向量, 有可能需要等待1-2分钟\n",
    "import time\n",
    "start  = time.time()\n",
    "with open(\"embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))\n",
    "end = time.time()\n",
    "\n",
    "print (\"花费时间：\",(end - start)/60,\"mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "花费时间: 1.3036367893218994 mins\n"
     ]
    }
   ],
   "source": [
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "time1 = time.time()\n",
    "cn_model = KeyedVectors.load_word2vec_format('embeddings/sgns.zhihu.bigram',\n",
    "                                             binary=False, unicode_errors=\"ignore\")\n",
    "time2 = time.time()\n",
    "print(\"花费时间:\",(time2 - time1)/60,'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词向量模型**  \n",
    "在这个词向量模型里，每一个词是一个索引，对应的是一个长度为300的向量，我们今天需要构建的LSTM神经网络模型并不能直接处理汉字文本，需要先进行分次并把词汇转换为词向量，步骤请参考下图，步骤的讲解会跟着代码一步一步来，如果你不知道RNN，GRU，LSTM是什么，我推荐deeplearning.ai的课程，网易公开课有免费中文字幕版，但我还是推荐有习题和练习代码部分的的coursera原版：  \n",
    "<img src='flowchart.jpg' style='width:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity for Vector Space Models by Christian S. Perone\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算相似度 - cos(x,y)\n",
    "cn_model.similarity('橘子', '橙子')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4895461"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.similarity('国王','王子')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上面的相识度的计算是按照余弦相识度的计算实现：cos(x,y) = (x.y)/(abs(x)*abs(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cn_model['橘子'] => 是一个1*embedding_dims 的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66128117"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot（'橘子'/|'橘子'|， '橙子'/|'橙子'| ）\n",
    "np.dot(cn_model['橘子']/np.linalg.norm(cn_model['橘子']), \n",
    "cn_model['橙子']/np.linalg.norm(cn_model['橙子']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('图形学', 0.6659187078475952),\n",
       " ('计算机技术', 0.6460264921188354),\n",
       " ('软件工程', 0.6406547427177429),\n",
       " ('微电子', 0.6350466012954712),\n",
       " ('自动控制', 0.6313568353652954),\n",
       " ('机械工程', 0.6216508150100708),\n",
       " ('数字电路', 0.6193503141403198),\n",
       " ('通信工程', 0.6179863214492798),\n",
       " ('计算机语言', 0.6152951717376709),\n",
       " ('计算机科学', 0.6142673492431641)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出最相近的词，余弦相似度\n",
    "cn_model.most_similar(positive=['计算机'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 歹徒 老师 会计师 程序员 律师 医生  中:\n",
      "不是同一类别的词为: 歹徒\n"
     ]
    }
   ],
   "source": [
    "# 找出不同的词\n",
    "test_words = '歹徒 老师 会计师 程序员 律师 医生 '\n",
    "test_words_result = cn_model.doesnt_match(test_words.split())\n",
    "print('在 '+test_words+' 中:\\n不是同一类别的词为: %s' %test_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('帅哥', 0.5870201587677002),\n",
       " ('美人', 0.5252472162246704),\n",
       " ('大美女', 0.5057994723320007),\n",
       " ('靓女', 0.48884159326553345),\n",
       " ('丑男', 0.4734836220741272),\n",
       " ('美人儿', 0.4580216407775879)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.most_similar(positive=['女人','美女'], negative=['男人'], topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引和词语之间的对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7224, '宾馆')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_model.vocab[\"宾馆\"].index,cn_model.index2word[7224]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ！！乱入 加载10w条微博2分类数据样本 =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>美~~~~~[爱你]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>梦想有多大，舞台就有多大![鼓掌]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1              ﻿更博了，爆照了，帅的呀，就是越来越爱你！生快傻缺[爱你][爱你][爱你]\n",
       "1      1  @张晓鹏jonathan 土耳其的事要认真对待[哈哈]，否则直接开除。@丁丁看世界 很是细心...\n",
       "2      1  姑娘都羡慕你呢…还有招财猫高兴……//@爱在蔓延-JC:[哈哈]小学徒一枚，等着明天见您呢/...\n",
       "3      1                                         美~~~~~[爱你]\n",
       "4      1                                  梦想有多大，舞台就有多大![鼓掌]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###加载数据集合####\n",
    "import pandas as pd\n",
    "path = './git下载的数据集合/weibo_senti_100k/weibo_senti_100k.csv'\n",
    "data_pd = pd.read_csv(path)\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 现在我们将所有的评价内容放置到一个list里\n",
    "# 这里和视频课程不一样, 因为很多同学反应按照视频课程里的读取方法会乱码,\n",
    "# 经过检查发现是原始文本里的编码是gbk造成的,\n",
    "# 这里我进行了简单的预处理, 以避免乱码\n",
    "train_texts_orig = []\n",
    "# 文本所对应的labels, 也就是标记\n",
    "train_target = []\n",
    "pos_length = []\n",
    "neg_length  = []\n",
    "with open(\"positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        pos_length.append(len(dic[\"text\"]))\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])\n",
    "\n",
    "with open(\"negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        dic = eval(line)\n",
    "        neg_length.append(len(dic[\"text\"]))\n",
    "        train_texts_orig.append(dic[\"text\"])\n",
    "        train_target.append(dic[\"label\"])\n",
    "\n",
    "# 新增加的数据样本 - 1403个样本，其中正面样本 neg_nums+pos_nums = 515 + 888 \n",
    "with open(\"./Newdatas/mt_217356_Filters.txt\", \"r\", encoding=\"utf-8\") as f1: # 每个样本长度大约50\n",
    "    lines = f1.readlines()\n",
    "    for line in lines:\n",
    "        item= line.split('/')\n",
    "        train_target.append(item[0])\n",
    "        train_texts_orig.append(item[1])\n",
    "        if item[0] == '1':\n",
    "            pos_length.append(1)\n",
    "        else:\n",
    "            neg_length.append(0)\n",
    "        \n",
    "# 新增加的数据-\n",
    "with open(\"./Newdatas/newdatas/hotel/pos.txt\",'r',encoding= 'utf-8') as f2:\n",
    "    for line in f2.readlines():\n",
    "        if len(line)>50:\n",
    "            train_texts_orig.append(line)\n",
    "            train_target.append(1)\n",
    "            pos_length.append(len(line))\n",
    "\n",
    "with open(\"./Newdatas/newdatas/hotel/neg.txt\",'r',encoding= 'utf-8') as f3:\n",
    "    for line in f3.readlines():\n",
    "        if len(line)>50:\n",
    "            train_texts_orig.append(line)\n",
    "            train_target.append(0)\n",
    "            neg_length.append(len(line))\n",
    "\n",
    "## 新增加的美团爬取的数据，正面样本2347，负面样本1603，长度已经大约50\n",
    "# postive \n",
    "with open(\"./Newdatas/meituan/contentMergedPostive.txt\",'r',encoding = 'utf-8' ) as f4:\n",
    "    for line in f4.readlines():\n",
    "        train_texts_orig.append(line)\n",
    "        train_target.append(1)\n",
    "        pos_length.append(len(line))\n",
    "f4.close()\n",
    "\n",
    "# negative \n",
    "with open('./Newdatas/meituan/contentMergedNeg.txt','r',encoding = 'utf-8' ) as f4:\n",
    "    for line in f4.readlines():\n",
    "        train_texts_orig.append(line)\n",
    "        train_target.append(0)\n",
    "        neg_length.append(len(line))\n",
    "f4.close()\n",
    "\n",
    "# 添加的大众点评的数据 - 全是正面的 - 714\n",
    "# postive \n",
    "with open(\"./Newdatas/dianping/dianpindata_allmerged.txt\",'r',encoding = 'utf-8' ) as f5:\n",
    "    for line in f5.readlines():\n",
    "        train_texts_orig.append(line)\n",
    "        train_target.append(1)\n",
    "        pos_length.append(len(line))\n",
    "f5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target[8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'酒店地理位置：4分，地理位置一般，离火车站文殊院较近，出租车相对比较好叫，大约等5分钟左右。\\n\\n酒店环境：5分，很有味道的芙蓉文化主题酒店，很有特色，个人很喜欢，就是作为4星级酒店来说服务态度稍微差了一点。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是样本长度的大小统计 - 确定输入模型的文本长度时，并不是按照样本长度来计算的，而是下面的token[也就是已数据清洗以后的数据]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129.28333233729757, 0, 3008, 400.8825197827347, 16733, 9653, 7080)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = np.array(neg_length + pos_length)\n",
    "np.mean(lens),lens.min(),lens.max(),lens.mean() + 2*np.std(lens),len(lens),len(pos_length),len(neg_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "百分比: 92.0396820653798 %\n"
     ]
    }
   ],
   "source": [
    "means = lens.mean() + 2*np.std(lens)\n",
    "print (\"百分比:\",(np.sum(lens<300))/len(lens)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'说实话 初次到绍兴，住这个酒店比较好，为什么呢，因为离景点都比较近，条件还不错，房子比较有特色，只是楼道慢陡的，接送的师傅人比较好，服务员也还不错。\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[8900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"这个样本对应的标签label 值时: - 负面的评论标签设为零\"\n",
    "train_target[8900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下面的是正面样本：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'总的来说，这样的酒店配这样的价格还算可以，希望他赶快装修，给我的客人留些好的印象'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"下面的是正面样本：\")\n",
    "train_texts_orig[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"正面样本对应的label - 值等于一\"\n",
    "train_target[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本长度: 16733 ; 标签长度: 16733\n"
     ]
    }
   ],
   "source": [
    "print(\"样本长度:\",len(train_texts_orig),\";\",\"标签长度:\",len(train_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模  - 下面的代码可以执行\n",
    "#from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "#from tensorflow.contrib.keras.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "#from tensorflow.contrib.keras.python.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.contrib.keras.python.keras.preprocessing.sequence import pad_sequences\n",
    "#from tensorflow.contrib.keras.python.keras.optimizers import RMSprop\n",
    "#from tensorflow.contrib.keras.python.keras.optimizers import Adam\n",
    "#from tensorflow.contrib.keras.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 我们使用tensorflow的keras接口来建模 - 下面的代码在当前执行不通\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分词和tokenize**  \n",
    "首先我们去掉每个样本的标点符号，然后用jieba分词，jieba分词返回一个生成器，没法直接进行tokenize，所以我们将分词结果转换成一个list，并将它索引化[每一个词对应为一个索引数组，一个样本就对应成一段索引数字]，这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 进行分词和tokenize\n",
    "# train_tokens是一个长长的list，其中含有4000个小list，对应每一条评价\n",
    "train_tokens = []\n",
    "for text in train_texts_orig:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、\"\"【】~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 结巴分词\n",
    "    cut = jieba.cut(text)\n",
    "    # 结巴分词的输出结果为一个生成器\n",
    "    # 把生成器转换为list\n",
    "    cut_list = [ i for i in cut ]\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list[i] = 0\n",
    "    train_tokens.append(cut_list)\n",
    "# train_tokens 格式= [[样本1 索引序列],[样本2 索引序列],.....[样本n索引序列] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'服务差 早上退房前台跟欠她钱似的，开完发票，手机就往那一扔，发票开完直接往手机上一放告诉我开完了，太差了，服务行业一点服务都没有，马桶水按不下去，空调有噪音，座机没有电话线，再也不去了\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts_orig[4500]  # \"这个样本映射成下面一段索引序列\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1191,   710,  1001, 39390,  8241,   103,  4406,    47,   247,\n",
       "        2632,   264,   566, 12457,   300,    38,   478,    89,    41,\n",
       "        2073, 12457,   264,   566,   209,   478,   300,    24,     0,\n",
       "         362,     0,   566,     3,  5915,     3, 28158,   177,  1191,\n",
       "          11,    29,  6729,   548,   649,    10,   882,  2437,     8,\n",
       "        8344, 32243,    29, 52968,  1126,     0,     3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"上面的这句话对应的索引序列是，比如第一个词宾馆==>对应的index值=7224\"\n",
    "np.array(train_tokens[4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计词汇表大小：也就是统计样本中有多少个灭有重复出现的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小是： 28220\n"
     ]
    }
   ],
   "source": [
    "vocab_tokens = []\n",
    "for tokens in train_tokens:\n",
    "    for token in tokens:\n",
    "        if token not in vocab_tokens:\n",
    "            vocab_tokens.append(token)\n",
    "        else:\n",
    "            pass\n",
    "print(\"词汇表大小是：\",len(vocab_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**索引长度标准化**  \n",
    "因为每段评语的长度是不一样的，我们如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以我们取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 23,  86,  16, ..., 272, 115, 424])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [ len(tokens) for tokens in train_tokens ]\n",
    "num_tokens = np.array(num_tokens)\n",
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.0492440088448"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均tokens的长度\n",
    "np.mean(num_tokens) # \"相当于样本中的平均长度是71\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最长的评价tokens的长度\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGHCAYAAABiT1LUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6cf9be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.log(num_tokens), bins = 100)\n",
    "plt.xlim((0,10))\n",
    "plt.ylabel('number of tokens')\n",
    "plt.xlabel('length of tokens')\n",
    "plt.title('Distribution of tokens length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9591226916870854"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取tokens的长度为236时，大约95%的样本被涵盖\n",
    "# 我们对长度不足的进行padding，超长的进行修剪\n",
    "np.sum( num_tokens < max_tokens ) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**反向tokenize ==>将索引转换成文本内容**  \n",
    "我们定义一个function，用来把索引转换成可阅读的文本，这对于debug很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用来将tokens转换为文本\n",
    "def reverse_tokens(tokens):\n",
    "    text = ''\n",
    "    for i in tokens:\n",
    "        if i != 0:\n",
    "            text = text + cn_model.index2word[i]\n",
    "        else:\n",
    "            text = text + ' '\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reverse = reverse_tokens(train_tokens[4500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下可见，训练样本的极性并不是那么精准，比如说下面的样本，对早餐并不满意，但被定义为正面评价，这会迷惑我们的模型，不过我们暂时不对训练样本进行任何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'服务差早上退房前台跟欠她钱似的开完发票手机就往那一扔发票开完直接往手机上 告诉 完了太差了服务行业一点服务都没有马桶水按不下去空调有噪音座机没有电话线再也 了'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 经过tokenize再恢复成文本\n",
    "# 可见标点符号都没有了\n",
    "reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'服务差 早上退房前台跟欠她钱似的，开完发票，手机就往那一扔，发票开完直接往手机上一放告诉我开完了，太差了，服务行业一点服务都没有，马桶水按不下去，空调有噪音，座机没有电话线，再也不去了\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始文本\n",
    "train_texts_orig[4500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准备Embedding Matrix**  \n",
    "现在我们来为模型准备embedding matrix（词向量矩阵），根据keras的要求，我们需要准备一个维度为$(numwords, embeddingdim)$的矩阵，num words代表我们使用的词汇表的数量vocab_size，emdedding dimension在我们现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。  \n",
    "注意我们只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为我们的训练样本很小，一共只有4k，如果我们有100k，200k甚至更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面这个构建了词嵌入矩阵 Embedding Martix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 只使用前50000个词\n",
    "embedding_dim = 300  #\"词嵌入维度\"\n",
    "num_words     = 50000 # 词汇表大小\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = cn_model.index2word[1]\n",
    "#cn_model[words] == (embedding_matrix[1,:]) => is equal with each others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum( cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_matrix的维度，\n",
    "# 这个维度为keras的要求，后续会在模型中用到\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding（填充）和truncating（修剪）**  \n",
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了236这个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad前的长度: 83\n",
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0    290   3053\n",
      "     57    169     73      1     25  11216     49    163  15985 211356\n",
      "  55319     30      8  50197      1    228    223     40     35    653\n",
      " 113106      5   1642     29  11216   2751    500     98     30   3159\n",
      "   2225   2146    371   6285    169  27396      1   1191   5432   1080\n",
      "  20055     57    562      1  22671     40     35    169   2567      0\n",
      "  42665   7761    110      0  65932  41281  80655    110      0  35891\n",
      "    110      0  28781     57    169   1419      1  11670  80655  19470\n",
      "      1  58932  76788    169  35071     40    562     35  12398    657\n",
      "   4857]\n",
      "pad后的长度: 211\n"
     ]
    }
   ],
   "source": [
    "# 进行padding和truncating,输入的train_tokens是一个list\n",
    "# 返回的train_pad是一个numpy array\n",
    "print(\"pad前的长度:\",len(train_tokens[33]))\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,  # 将文本补齐成max_tokens 的样子\n",
    "                            padding='pre', truncating='pre')\n",
    "print(train_pad[33])\n",
    "print(\"pad后的长度:\",len(train_pad[33]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[ train_pad>=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "          290,   3053,     57,    169,     73,      1,     25,  11216,\n",
       "           49,    163,  15985, 211356,  55319,     30,      8,  50197,\n",
       "            1,    228,    223,     40,     35,    653, 113106,      5,\n",
       "         1642,     29,  11216,   2751,    500,     98,     30,   3159,\n",
       "         2225,   2146,    371,   6285,    169,  27396,      1,   1191,\n",
       "         5432,   1080,  20055,     57,    562,      1,  22671,     40,\n",
       "           35,    169,   2567,      0,  42665,   7761,    110,      0,\n",
       "        65932,  41281,  80655,    110,      0,  35891,    110,      0,\n",
       "        28781,     57,    169,   1419,      1,  11670,  80655,  19470,\n",
       "            1,  58932,  76788,    169,  35071,     40,    562,     35,\n",
       "        12398,    657,   4857])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可见padding之后前面的tokens全变成0，文本在最后面\n",
    "train_pad[33]  # 长度是max_tokens = 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备target向量，\n",
    "train_target = np.array(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,   #236 长度\n",
    "                                                    train_target,#236 长度\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               北京 山庄在北方 了其实偶尔也会向往一下南方的“江南风情 别致的江南小院静谧 着的那种江南味道令人着迷不过周末 上 江南那还是太远了些不过别担心咱北京也有这样的“江南风景北京 山庄位于 水长城附近 山庄小楼的设计和整个山庄的搭配真的给人  的感觉除了外部设计很具有江南风格外这里更有新意的是山庄的24 房是以24 命名 内都有店家自己手绘的 不说在这里过夜了 这种安静悠闲的环境中坐一下 茶  发呆那都不失为 享受啊 ：北京市 黄花  水长城\n",
      "class:  1\n"
     ]
    }
   ],
   "source": [
    "# 查看训练样本，确认无误\n",
    "print(reverse_tokens(X_train[3400]))\n",
    "print('class: ',y_train[3400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用keras搭建LSTM模型，模型的第一层是Embedding层，只有当我们把tokens索引转换为词向量矩阵之后，才可以用神经网络对文本进行处理。\n",
    "keras提供了Embedding接口，避免了繁琐的稀疏矩阵操作。   \n",
    "在Embedding层我们输入的矩阵为：$$(batchsize, maxtokens)$$\n",
    "输出矩阵为： $$(batchsize, maxtokens, embeddingdim)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用LSTM对样本进行分类\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArguments:\\ninput_dim: int > 0. Size of the vocabulary = nums_words = vocab_size \\noutput_dim: int >= 0. Dimension of the dense embedding/ embedding dims = 300\\nembeddings_initializer: Initializer for the `embeddings` matrix.\\ninput_length:Length of input sequences, when it is constant.\\n |        This argument is required if you are going to connect\\n |        `Flatten` then `Dense` layers upstream\\n |        (without it, the shape of the dense outputs cannot be computed).\\n \\n Input shape:\\n |      2D tensor with shape: `(batch_size, sequence_length)\\n Output shape:\\n |      3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\\n \\n'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型第一层为embedding\n",
    "embedding_1st = Embedding(num_words, # num_words = 20000\n",
    "                    embedding_dim,   # embedding_dim = 300\n",
    "                    weights=[embedding_matrix], # embedding_matrix.shape = (20000,300)\n",
    "                    input_length=max_tokens,#max_tokens = 211\n",
    "                    trainable=False)\n",
    "model.add(embedding_1st)\n",
    "'''\n",
    "Arguments:\n",
    "input_dim: int > 0. Size of the vocabulary = nums_words = vocab_size \n",
    "output_dim: int >= 0. Dimension of the dense embedding/ embedding dims = 300\n",
    "embeddings_initializer: Initializer for the `embeddings` matrix.\n",
    "input_length:Length of input sequences, when it is constant.\n",
    " |        This argument is required if you are going to connect\n",
    " |        `Flatten` then `Dense` layers upstream\n",
    " |        (without it, the shape of the dense outputs cannot be computed).\n",
    " \n",
    " Input shape:\n",
    " |      2D tensor with shape: `(batch_size, sequence_length)\n",
    " Output shape:\n",
    " |      3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在2019年6月10日修改了一些大坑的bug, 可能是数据的顺序变了, \n",
    "# 结果模型训练的效果没有去年最早的时候效果好了, \n",
    "# 有兴趣的同学可以调整一下模型参数, 看看会不会有更好的结果\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**构建模型**  \n",
    "我在这个教程中尝试了几种神经网络结构，因为训练样本比较少，所以我们可以尽情尝试，训练过程等待时间并不长：  \n",
    "**GRU：**如果使用GRU的话，测试样本可以达到87%的准确率，但我测试自己的文本内容时发现，GRU最后一层激活函数的输出都在0.5左右，说明模型的判断不是很明确，信心比较低，而且经过测试发现模型对于否定句的判断有时会失误，我们期望对于负面样本输出接近0，正面样本接近1而不是都徘徊于0.5之间。  \n",
    "**BiLSTM：**测试了LSTM和BiLSTM，发现BiLSTM的表现最好，LSTM的表现略好于GRU，这可能是因为BiLSTM对于比较长的句子结构有更好的记忆，有兴趣的朋友可以深入研究一下。  \n",
    "Embedding之后第，一层我们用BiLSTM返回sequences，然后第二层16个单元的LSTM不返回sequences，只返回最终结果，最后是一个全链接层，用sigmoid激活函数输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRU的代码\n",
    "#model.add(GRU(units=32, return_sequences=True))\n",
    "#model.add(GRU(units=16, return_sequences=True))\n",
    "#model.add(GRU(units=4, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "optimizer = Adam(lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面embedding 的输出维度是 128X236X300; 双向LSTM的输出维度(隐藏节点个数units=64) NoneXNoneX128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM_cell（隐藏节点=16）,输出的维度为NoneX16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 211, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 128)         186880    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 16)                9280      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,196,177\n",
      "Trainable params: 196,177\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 我们来看一下模型的结构，一共90k左右可训练的变量\n",
    "model.summary()  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "path_checkpoint = 'sentiment_checkpoint.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 尝试加载已训练模型\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12047 samples, validate on 1339 samples\n",
      "Epoch 1/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.2592 - acc: 0.8974Epoch 00000: val_loss improved from inf to 0.17877, saving model to sentiment_checkpoint.keras\n",
      "12047/12047 [==============================] - 204s - loss: 0.2595 - acc: 0.8974 - val_loss: 0.1788 - val_acc: 0.9425\n",
      "Epoch 2/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9544Epoch 00001: val_loss did not improve\n",
      "\n",
      "Epoch 00001: reducing learning rate to 0.0009999999776482583.\n",
      "12047/12047 [==============================] - 202s - loss: 0.1429 - acc: 0.9543 - val_loss: 0.2003 - val_acc: 0.9305\n",
      "Epoch 3/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9688Epoch 00002: val_loss improved from 0.17877 to 0.16981, saving model to sentiment_checkpoint.keras\n",
      "12047/12047 [==============================] - 204s - loss: 0.1045 - acc: 0.9689 - val_loss: 0.1698 - val_acc: 0.9500\n",
      "Epoch 4/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9764Epoch 00003: val_loss improved from 0.16981 to 0.16882, saving model to sentiment_checkpoint.keras\n",
      "12047/12047 [==============================] - 202s - loss: 0.0866 - acc: 0.9764 - val_loss: 0.1688 - val_acc: 0.9507\n",
      "Epoch 5/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0784 - acc: 0.9791Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 9.999999310821295e-05.\n",
      "12047/12047 [==============================] - 201s - loss: 0.0784 - acc: 0.9791 - val_loss: 0.1707 - val_acc: 0.9522\n",
      "Epoch 6/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9807Epoch 00005: val_loss did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 9.999999019782991e-06.\n",
      "12047/12047 [==============================] - 202s - loss: 0.0720 - acc: 0.9807 - val_loss: 0.1712 - val_acc: 0.9522\n",
      "Epoch 7/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9808Epoch 00006: val_loss did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 9.99999883788405e-07.\n",
      "12047/12047 [==============================] - 201s - loss: 0.0713 - acc: 0.9808 - val_loss: 0.1713 - val_acc: 0.9522\n",
      "Epoch 8/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9808Epoch 00007: val_loss did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 9.99999883788405e-08.\n",
      "12047/12047 [==============================] - 201s - loss: 0.0712 - acc: 0.9808 - val_loss: 0.1713 - val_acc: 0.9522\n",
      "Epoch 9/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9808Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 1e-08.\n",
      "12047/12047 [==============================] - 201s - loss: 0.0712 - acc: 0.9808 - val_loss: 0.1713 - val_acc: 0.9522\n",
      "Epoch 10/200\n",
      "12032/12047 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9808Epoch 00009: val_loss did not improve\n",
      "12047/12047 [==============================] - 200s - loss: 0.0712 - acc: 0.9808 - val_loss: 0.1713 - val_acc: 0.9522\n",
      "Epoch 00009: early stopping\n",
      "Fit Training Times: 33.713078276316324 mins\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "start =time.time()\n",
    "model.fit(X_train, y_train,\n",
    "          validation_split=0.1, \n",
    "          epochs=200,\n",
    "          batch_size=128,\n",
    "          callbacks=callbacks)\n",
    "end =time.time()\n",
    "print(\"Fit Training Times:\",(end -start)/60,\"mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**结论**  \n",
    "我们首先对测试样本进行预测，得到了还算满意的准确度。  \n",
    "之后我们定义一个预测函数，来预测输入的文本的极性，可见模型对于否定句和一些简单的逻辑结构都可以进行准确的判断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3347/3347 [==============================] - 21s    \n",
      "Accuracy:95.31%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    print(text)\n",
    "    # 去标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # 分词\n",
    "    cut = jieba.cut(text)\n",
    "    cut_list = [ i for i in cut ]\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_list):\n",
    "        try:\n",
    "            cut_list[i] = cn_model.vocab[word].index\n",
    "            if cut_list[i] >= 50000:\n",
    "                cut_list[i] = 0\n",
    "        except KeyError:\n",
    "            cut_list[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_list], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # 预测\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('是一例正面评价','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('是一例负面评价','output=%.2f'%coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "酒店设施不是新的，服务态度很不好\n",
      "是一例负面评价 output=0.01\n",
      "酒店卫生条件非常不好\n",
      "是一例负面评价 output=0.01\n",
      "床铺非常舒适\n",
      "是一例正面评价 output=0.95\n",
      "房间很凉，不给开暖气\n",
      "是一例负面评价 output=0.26\n",
      "房间很凉爽，空调冷气很足\n",
      "是一例正面评价 output=0.99\n",
      "酒店环境不好，住宿体验很不好\n",
      "是一例负面评价 output=0.01\n",
      "房间隔音不到位\n",
      "是一例负面评价 output=0.32\n",
      "晚上回来发现没有打扫卫生\n",
      "是一例正面评价 output=0.59\n",
      "因为过节所以要我临时加钱，比团购的价格贵\n",
      "是一例负面评价 output=0.04\n",
      "空调不好用，外面的窗帘也是脏的\n",
      "是一例负面评价 output=0.00\n",
      "位置在密云，还算比较好找。酒店在一个山脚下，像很多别墅的感觉。从酒店内可以有栈道直接登山。酒店里有十二种不同温泉，分别对应圆明园十二兽首。每种温泉的功效不同，人参、雄黄、芦荟，藏红花等等，看上去挺补的。酒店的自助特别棒，尤其是现烤的牛排和红烩海鲜很赞\n",
      "是一例正面评价 output=0.99\n",
      "充电器和泳衣不慎落在了酒店 回家才想起来 联系了宾客服务中心 帮忙找到并顺丰到付 非常感谢\n",
      "是一例正面评价 output=0.92\n",
      "刷房卡就餐 自助早餐偏中式更多 个人感觉一般 周末人比较多\n",
      "是一例负面评价 output=0.40\n",
      "门口的安保人员很负责；入住后礼宾部会主动把行李送到客房 并对酒店的环境设施进行介绍；中午会给车辆挡风玻璃罩上遮阳板\n",
      "是一例正面评价 output=0.99\n",
      "今天的天气不错，很适合出去玩耍，但是路上堵车了，我们在车里有说有笑也很开心\n",
      "是一例正面评价 output=1.00\n",
      "我们在酒店尽情看着电影，但是厕所漏水了\n",
      "是一例正面评价 output=0.97\n",
      "酒店好评\n",
      "是一例正面评价 output=0.96\n",
      "真的很不错，主要是宽敞，环境安逸住得舒服\n",
      "是一例正面评价 output=0.99\n",
      "北京三里屯通盈中心洲际酒店，位置不错对面就是太古里，周边有商场、餐饮、酒吧，白天还是晩上都非常的热闹\n",
      "是一例正面评价 output=0.99\n"
     ]
    }
   ],
   "source": [
    "test_list = [\n",
    "    '酒店设施不是新的，服务态度很不好',\n",
    "    '酒店卫生条件非常不好',\n",
    "    '床铺非常舒适',\n",
    "    '房间很凉，不给开暖气',\n",
    "    '房间很凉爽，空调冷气很足',\n",
    "    '酒店环境不好，住宿体验很不好',\n",
    "    '房间隔音不到位' ,\n",
    "    '晚上回来发现没有打扫卫生',\n",
    "    '因为过节所以要我临时加钱，比团购的价格贵',\n",
    "    '空调不好用，外面的窗帘也是脏的',\n",
    "    '位置在密云，还算比较好找。酒店在一个山脚下，像很多别墅的感觉。从酒店内可以有栈道直接登山。酒店里有十二种不同温泉，分别对应圆明园十二兽首。每种温泉的功效不同，人参、雄黄、芦荟，藏红花等等，看上去挺补的。酒店的自助特别棒，尤其是现烤的牛排和红烩海鲜很赞',\n",
    "    '充电器和泳衣不慎落在了酒店 回家才想起来 联系了宾客服务中心 帮忙找到并顺丰到付 非常感谢',\n",
    "    '刷房卡就餐 自助早餐偏中式更多 个人感觉一般 周末人比较多',\n",
    "    '门口的安保人员很负责；入住后礼宾部会主动把行李送到客房 并对酒店的环境设施进行介绍；中午会给车辆挡风玻璃罩上遮阳板',\n",
    "    '今天的天气不错，很适合出去玩耍，但是路上堵车了，我们在车里有说有笑也很开心',\n",
    "    '我们在酒店尽情看着电影，但是厕所漏水了',\n",
    "    '酒店好评',\n",
    "    '真的很不错，主要是宽敞，环境安逸住得舒服',\n",
    "    '北京三里屯通盈中心洲际酒店，位置不错对面就是太古里，周边有商场、餐饮、酒吧，白天还是晩上都非常的热闹'\n",
    "]\n",
    "\n",
    "for text in test_list:\n",
    "    predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**错误分类的文本**\n",
    "经过查看，发现错误分类的文本的含义大多比较含糊，就算人类也不容易判断极性，如index为101的这个句子，好像没有一点满意的成分，但这例子评价在训练样本中被标记成为了正面评价，而我们的模型做出的负面评价的预测似乎是合理的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.T[0]\n",
    "y_pred = [1 if p>= 0.5 else 0 for p in y_pred]\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找出错误分类的索引\n",
    "misclassified = np.where( y_pred != y_actual )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141\n"
     ]
    }
   ],
   "source": [
    "# 输出所有错误分类的索引\n",
    "len(misclassified)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          7月19日入住酒店总体感觉较好不足之处如下1   的指示标志不明显而且要走很远2服务人员似乎不知道什么是主动见面打招呼 比起海景 要差的远我在酒店大厅走廊院子里见到酒店各个岗位的服务人员大部分都不知道应该主动和客人打招呼问好\n",
      "预测的分类 1\n",
      "实际的分类 1\n"
     ]
    }
   ],
   "source": [
    "# 我们来找出错误分类的样本看看\n",
    "idx=101\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                     在携程预定酒店这么多次遇到的 的酒店住在行政豪华房早上7 起来连洗澡的热水也没有 时说没有发票请携程帮帮忙不要把这种酒店放在你们网上害人谢谢\n",
      "预测的分类 0\n",
      "实际的分类 0\n"
     ]
    }
   ],
   "source": [
    "idx=323\n",
    "print(reverse_tokens(X_test[idx]))\n",
    "print('预测的分类', y_pred[idx])\n",
    "print('实际的分类', y_actual[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
